{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import dirname, join, splitext\n",
    "from pybrain.datasets import SequentialDataSet\n",
    "from itertools import cycle\n",
    "from pybrain.tools.shortcuts import buildNetwork\n",
    "from pybrain.structure.modules import LSTMLayer\n",
    "from pybrain.supervised import RPropMinusTrainer\n",
    "from sys import stdout\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time as tm\n",
    "from numba import jit\n",
    "import random\n",
    "\n",
    "atm=tm.time()\n",
    "data=[]\n",
    "\n",
    "for i in stock_dat:\n",
    "    data=getdata(i)\n",
    "    print tm.time()-atm\n",
    "    train(data,i)\n",
    "    print tm.time()-atm\n",
    "#plt.plot(range(0, EPOCHS, EPOCHS_PER_CYCLE), train_errors)\n",
    "#plt.xlabel('epoch')\n",
    "#plt.ylabel('error')\n",
    "#plt.show()\n",
    "#for sample, target in ds.getSequenceIterator(0):\n",
    " #   print(\"               sample = %4.1f\" % sample)\n",
    "  #  print(\"predicted next sample = %4.1f\" % net.activate(sample))\n",
    " #   print(\"   actual next sample = %4.1f\" % target)\n",
    "  #  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ECL', 'EW', 'FLS', 'GPC', 'HOG', 'INTU', 'JWN', 'LLL', 'MAS', 'MON', 'NEE', 'NYX', 'PEG', 'PPG', 'RHT', 'SJM', 'SWN', 'TMO', 'USB', 'WFM', 'XOM']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## list of stock to train\n",
    "stock_dat=[ 'ECL', 'EW', 'FLS', 'GPC', 'HOG', 'INTU', 'JWN', 'LLL', 'MAS', 'MON', 'NEE', 'NYX', 'PEG', 'PPG', 'RHT', 'SJM', 'SWN', 'TMO', 'USB', 'WFM', 'XOM']\n",
    "print stock_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the training functions\n",
    "from pybrain.supervised import BackpropTrainer\n",
    "from numba import jit\n",
    "import pandas as pd\n",
    "@jit\n",
    "def getdata(ticker1):\n",
    "    data_dir = join(dirname('/home/elisha/stock_applet/'), \"daily\")\n",
    "    fname = join(data_dir, \"table_%s.csv\" % ticker1.lower())\n",
    "    a=open(fname,'r')\n",
    "    b=a.read().split()\n",
    "    for i in b :\n",
    "            x=i.split(',')\n",
    "            data.append(float(x[2])) \n",
    "    return data\n",
    "\n",
    "@jit\n",
    "def train(data,name):\n",
    "    ds = SequentialDataSet(1, 1)\n",
    "    for sample, next_sample in zip(data, cycle(data[1:])):\n",
    "        ds.addSample(sample, next_sample)\n",
    "    net = buildNetwork(1, 200, 1, hiddenclass=LSTMLayer, outputbias=False, recurrent=True)\n",
    "\n",
    "    trainer = RPropMinusTrainer(net, dataset=ds)\n",
    "    train_errors = [] # save errors for plotting later\n",
    "    EPOCHS_PER_CYCLE = 5\n",
    "    CYCLES = 20\n",
    "    EPOCHS = EPOCHS_PER_CYCLE * CYCLES\n",
    "    store=[]\n",
    "    for i in xrange(CYCLES):\n",
    "        trainer.trainEpochs(EPOCHS_PER_CYCLE)\n",
    "        train_errors.append(trainer.testOnData())\n",
    "        epoch = (i+1) * EPOCHS_PER_CYCLE\n",
    "        print(\"\\r epoch {}/{}\".format(epoch, EPOCHS))\n",
    "        print tm.time()-atm\n",
    "        stdout.flush() \n",
    "    for sample, target in ds.getSequenceIterator(0):\n",
    "        store.append(net.activate(sample))\n",
    "    abcd=pd.DataFrame(store)\n",
    "    abcd.to_csv(\"/home/elisha/stock_applet/lstmdata/\"+name+\".csv\",encoding='utf-8')\n",
    "    print \"result printed to file\"\n",
    "@jit\n",
    "def train_1(data):\n",
    "    ds = SequentialDataSet(1, 1)\n",
    "    for sample, next_sample in zip(data, cycle(data[1:])):\n",
    "        ds.addSample(sample, next_sample)\n",
    "    net = buildNetwork(1, 150, 1, hiddenclass=LSTMLayer, outputbias=False, recurrent=True)\n",
    "\n",
    "    trainer = RPropMinusTrainer(net, dataset=ds)\n",
    "    train_errors = [] # save errors for plotting later\n",
    "    EPOCHS_PER_CYCLE = 5\n",
    "    CYCLES = 10\n",
    "    EPOCHS = EPOCHS_PER_CYCLE * CYCLES\n",
    "    for i in xrange(CYCLES):\n",
    "        trainer.trainEpochs(EPOCHS_PER_CYCLE)\n",
    "        train_errors.append(trainer.testOnData())\n",
    "        epoch = (i+1) * EPOCHS_PER_CYCLE\n",
    "        print(\"\\r epoch {}/{}\".format(epoch, EPOCHS))\n",
    "        print tm.time()-atm\n",
    "        stdout.flush() \n",
    "    print(\"final error =\", train_errors[-1])\n",
    "@jit\n",
    "def train_bprop(data):\n",
    "    ds = SequentialDataSet(1, 1)\n",
    "    for sample, next_sample in zip(data, cycle(data[1:])):\n",
    "        ds.addSample(sample, next_sample)\n",
    "    net = buildNetwork(1, 150, 1, hiddenclass=LSTMLayer, outputbias=False, recurrent=True)\n",
    "\n",
    "    trainer = BackpropTrainer(net,ds)\n",
    "    train_errors = [] # save errors for plotting later\n",
    "    trainer.trainUntilConvergence(maxEpochs=100)\n",
    "    print tm.time()-atm\n",
    "    for sample, target in ds.getSequenceIterator(0):\n",
    "        print(\"               sample = %4.1f\" % sample)\n",
    "        print(\"predicted next sample = %4.1f\" % net.activate(sample))\n",
    "        print(\"   actual next sample = %4.1f\" % target)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
